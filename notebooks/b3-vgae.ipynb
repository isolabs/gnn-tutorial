{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Auto-Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This tutorial demonstrates the use of Graph Auto-Encoders (GAEs) and Variational Graph Auto-Encoders (VGAEs) to perform unsupervised learning on a graph, as first implemented by <a href='https://arxiv.org/pdf/1611.07308.pdf'>Kipf (2016)</a>. The models will be implemented with PyTorch and <a href='https://pytorch-geometric.readthedocs.io/en/latest/index.html'>PyTorch Geometric</a> (a <a href='https://github.com/tkipf/gae'>tensorflow implementation</a> has been produced by the author of VGAE).\n",
    "To demonstrate the suitably of the models' embeddings, we show how unsupervised training using only the reconstruction loss improves performance on a link prediction task that the model is not directly optimising for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import igraph as ig\n",
    "import random\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from torch_geometric.nn import GCNConv, VGAE\n",
    "from torch_geometric.datasets import *\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch_geometric.nn.models.autoencoder import EPS, MAX_LOGVAR\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.inits import reset\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.utils import (negative_sampling, remove_self_loops, add_self_loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb6e826ba10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seed random number generator for reproducibility\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to save datasets\n",
    "fp_data = \"./datasets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubMed\n",
    "The PubMed graph is accessible through PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset - small download on first run\n",
    "dataset_name = \"PubMed\"  # options: 'Citeseer', 'Cora', 'PubMed'\n",
    "dataset = Planetoid(root=\"./datasets/\"+dataset_name, name=dataset_name)\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num nodes = 19717\n",
      "num edges = 88648\n",
      "num features = 500\n"
     ]
    }
   ],
   "source": [
    "# explore key features of the graph\n",
    "print('num nodes =', data.num_nodes)\n",
    "print('num edges =', data.num_edges)\n",
    "print('num features =', data.num_node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Useful PyG Data class attributes</strong>\n",
    "<table width=\"75%\">\n",
    "    <header>\n",
    "        <th style=\"text-align:left\">Name</th>\n",
    "        <th style=\"text-align:left\">Attribute</th>\n",
    "        <th style=\"text-align:left\">Shape</th>\n",
    "    </header>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">Node Feature Matrix</td>\n",
    "        <td style=\"text-align:left\">data.x</td>\n",
    "        <td style=\"text-align:left\">(num nodes, num features)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">Edge Index</td>\n",
    "        <td style=\"text-align:left\">data.edge_index</td>\n",
    "        <td style=\"text-align:left\">(2, num edges)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\">Node Classes</td>\n",
    "        <td style=\"text-align:left\">data.y</td>\n",
    "        <td style=\"text-align:left\">(num nodes, )</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a plot function using networkx\n",
    "def plot(data, outfile='./graph.eps'):\n",
    "    \"\"\"\n",
    "    plot a PyG graph using igraph\n",
    "    :param data: instance of torch_geometric.data\n",
    "    \"\"\"\n",
    "    assert data.edge_index is not None, 'cannot plot after train_test_split_edges has been called'\n",
    "    g = ig.Graph()\n",
    "    # set the number of vertices\n",
    "    g.add_vertices(data.num_nodes)\n",
    "    # get the edges as a list of tuples [(source, target) ...]\n",
    "    edges = list(zip(*tuple(data.edge_index)))\n",
    "    edges = [(int(a), int(b)) for a, b in edges]\n",
    "    g.add_edges(edges)\n",
    "    n_classes = len(np.unique(data.y))\n",
    "    # randomly generate a hexadecimal colour for each class\n",
    "    r = lambda: random.randint(0, 255)\n",
    "    colours = ['#%02X%02X%02X' % (r(),r(),r()) for i in range(n_classes)]\n",
    "    g.vs['color'] = [colours[cls] for cls in data.y]\n",
    "    # save the plot to a .eps file\n",
    "    ig.plot(\n",
    "        g, outfile,\n",
    "        layout=g.layout_auto(), vertex_size=15, vertex_colour='#D1D1D1',\n",
    "        edge_curved=True, bbox=(3000, 3000), margin=20\n",
    "    )\n",
    "    # read and show the plot\n",
    "    img = Image.open(outfile)\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the graph - ok to run on cora/citeseer but slower on pubmed/reddit etc\n",
    "if data.num_nodes < 5000:\n",
    "    plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for edge prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train edges 75352\n"
     ]
    }
   ],
   "source": [
    "# To perform edge prediction, we need to split the graph's edges intro train and test sets\n",
    "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
    "data = train_test_split_edges(data)\n",
    "# Note that data.edge_index is no longer available, as edges have been distributed between train and test\n",
    "print('num train edges', data.train_pos_edge_index.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Get the device to use for training our model\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('CUDA available:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then register the relevant data to the device in use\n",
    "x = data.x.to(dev)  # feature matrix\n",
    "train_pos_edge_index = data.train_pos_edge_index.to(dev)  # index of true positive edges in the train set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAE Model\n",
    "To understand GAEs, we show the models as constructed by PyG. Alternatively, these can be imported directly from torch_geometric.nn\n",
    " \n",
    "Importantly, the GAE class includes methods to:\n",
    "<ul>\n",
    "    <li>apply the encoder and decoder</li>\n",
    "    <li>calculate reconstruction loss</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the GAE class\n",
    "\n",
    "class GAE(torch.nn.Module):\n",
    "    r\"\"\"The Graph Auto-Encoder model from the\n",
    "    `\"Variational Graph Auto-Encoders\" <https://arxiv.org/abs/1611.07308>`_\n",
    "    paper based on user-defined encoder and decoder models.\n",
    "\n",
    "    Args:\n",
    "        encoder (Module): The encoder module.\n",
    "        decoder (Module, optional): The decoder module. If set to :obj:`None`,\n",
    "            will default to the\n",
    "            :class:`torch_geometric.nn.models.InnerProductDecoder`.\n",
    "            (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder=None):\n",
    "        super(GAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = InnerProductDecoder() if decoder is None else decoder\n",
    "        GAE.reset_parameters(self)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.encoder)\n",
    "        reset(self.decoder)\n",
    "\n",
    "\n",
    "    def encode(self, *args, **kwargs):\n",
    "        r\"\"\"Runs the encoder and computes node-wise latent variables.\"\"\"\n",
    "        return self.encoder(*args, **kwargs)\n",
    "\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        r\"\"\"Runs the decoder and computes edge probabilities.\"\"\"\n",
    "        return self.decoder(*args, **kwargs)\n",
    "\n",
    "\n",
    "    def recon_loss(self, z, pos_edge_index):\n",
    "        r\"\"\"Given latent variables :obj:`z`, computes the binary cross\n",
    "        entropy loss for positive edges :obj:`pos_edge_index` and negative\n",
    "        sampled edges.\n",
    "\n",
    "        Args:\n",
    "            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n",
    "            pos_edge_index (LongTensor): The positive edges to train against.\n",
    "        \"\"\"\n",
    "\n",
    "        pos_loss = -torch.log(\n",
    "            self.decoder(z, pos_edge_index, sigmoid=True) + EPS).mean()\n",
    "\n",
    "        # Do not include self-loops in negative samples\n",
    "        pos_edge_index, _ = remove_self_loops(pos_edge_index)\n",
    "        pos_edge_index, _ = add_self_loops(pos_edge_index)\n",
    "\n",
    "        neg_edge_index = negative_sampling(pos_edge_index, z.size(0))\n",
    "        neg_loss = -torch.log(1 -\n",
    "                              self.decoder(z, neg_edge_index, sigmoid=True) +\n",
    "                              EPS).mean()\n",
    "\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "\n",
    "    def test(self, z, pos_edge_index, neg_edge_index):\n",
    "        r\"\"\"Given latent variables :obj:`z`, positive edges\n",
    "        :obj:`pos_edge_index` and negative edges :obj:`neg_edge_index`,\n",
    "        computes area under the ROC curve (AUC) and average precision (AP)\n",
    "        scores.\n",
    "\n",
    "        Args:\n",
    "            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n",
    "            pos_edge_index (LongTensor): The positive edges to evaluate\n",
    "                against.\n",
    "            neg_edge_index (LongTensor): The negative edges to evaluate\n",
    "                against.\n",
    "        \"\"\"\n",
    "        pos_y = z.new_ones(pos_edge_index.size(1))\n",
    "        neg_y = z.new_zeros(neg_edge_index.size(1))\n",
    "        y = torch.cat([pos_y, neg_y], dim=0)\n",
    "\n",
    "        pos_pred = self.decoder(z, pos_edge_index, sigmoid=True)\n",
    "        neg_pred = self.decoder(z, neg_edge_index, sigmoid=True)\n",
    "        pred = torch.cat([pos_pred, neg_pred], dim=0)\n",
    "\n",
    "        y, pred = y.detach().cpu().numpy(), pred.detach().cpu().numpy()\n",
    "\n",
    "        return roc_auc_score(y, pred), average_precision_score(y, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the encoder and decoder that GAE will use\n",
    "# this code is slightly adapted from the PyG example to accept model_name\n",
    "class GAEEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GAEEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True)\n",
    "        self.conv2 = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        return self.conv2(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To decode embeddings and attempt input reconstruction, we can use the standard InnerProductDecoder\n",
    "class InnerProductDecoder(torch.nn.Module):\n",
    "    r\"\"\"The inner product decoder from the `\"Variational Graph Auto-Encoders\"\n",
    "    <https://arxiv.org/abs/1611.07308>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\sigma(\\mathbf{Z}\\mathbf{Z}^{\\top})\n",
    "\n",
    "    where :math:`\\mathbf{Z} \\in \\mathbb{R}^{N \\times d}` denotes the latent\n",
    "    space produced by the encoder.\"\"\"\n",
    "    def forward(self, z, edge_index, sigmoid=True):\n",
    "        r\"\"\"Decodes the latent variables :obj:`z` into edge probabilities for\n",
    "        the given node-pairs :obj:`edge_index`.\n",
    "\n",
    "        Args:\n",
    "            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n",
    "            sigmoid (bool, optional): If set to :obj:`False`, does not apply\n",
    "                the logistic sigmoid function to the output.\n",
    "                (default: :obj:`True`)\n",
    "        \"\"\"\n",
    "        value = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=1)\n",
    "        return torch.sigmoid(value) if sigmoid else value\n",
    "\n",
    "    def forward_all(self, z, sigmoid=True):\n",
    "        r\"\"\"Decodes the latent variables :obj:`z` into a probabilistic dense\n",
    "        adjacency matrix.\n",
    "\n",
    "        Args:\n",
    "            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n",
    "            sigmoid (bool, optional): If set to :obj:`False`, does not apply\n",
    "                the logistic sigmoid function to the output.\n",
    "                (default: :obj:`True`)\n",
    "        \"\"\"\n",
    "        adj = torch.matmul(z, z.t())\n",
    "        return torch.sigmoid(adj) if sigmoid else adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring the encoder and decoder together to compile the GAE\n",
    "# in_channels (the dimensionality of the input data) is simply the number of features\n",
    "# we define out_channels (the dimensionality of the latent vector or embedding) as 16\n",
    "gae = GAE(GAEEncoder(in_channels=dataset.num_features, out_channels=16)).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer with a learning rate of 0.01\n",
    "gae_optimizer = torch.optim.Adam(gae.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a writer to log training results\n",
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we will train the GAE in an unsupervised manner (using reconstruction loss) <i>we can validate it's performance by testing it's ability to perform link prediction during training </i> (even though this is not used in back propagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to test our predictions against the ground truth edges\n",
    "def test(model, pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x, train_pos_edge_index)\n",
    "    return model.test(z, pos_edge_index, neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "def train_gae():\n",
    "    print('training gae')\n",
    "    auc, ap = None, None\n",
    "    for epoch in range(0, 200):\n",
    "        gae_optimizer.zero_grad()\n",
    "        # get the embedding (z) of each node, shape(num_nodes, channels)\n",
    "        z = gae.encode(x, train_pos_edge_index)\n",
    "        # calculate the reconstruction loss to be used in backprop\n",
    "        loss = gae.recon_loss(z, train_pos_edge_index)\n",
    "        loss.backward()\n",
    "        gae_optimizer.step()\n",
    "\n",
    "        writer.add_scalar(\"l|oss\", loss.item(), epoch)\n",
    "\n",
    "        # calculate performance on edge prediction task\n",
    "        auc, ap = test(gae, data.test_pos_edge_index, data.test_neg_edge_index)\n",
    "        writer.add_scalar(\"AUC\", auc, epoch)\n",
    "        writer.add_scalar(\"AP\", ap, epoch)\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
    "    return auc, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_gae()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGAE Model\n",
    "To understand VGAEs, we again show the model as constructed by PyG (torch_geometric.nn.VGAE).\n",
    " \n",
    "VGAE differs from GAE in the use of two GCNs that encode the mean and variance. This allows it to reconstruct unseen inputs but requires a few updates to the main class.\n",
    "Namely:\n",
    "<ul>\n",
    "    <li>we use a KL loss, which uses the reconstruction loss from GAEs but adds a term to measure the probability distributions</li>\n",
    "    <li>the encoder uses the reparametrisation trick in order to maintain a differentiable loss function </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the VGAE class, which inherits from GAE \n",
    "\n",
    "class VGAE(GAE):\n",
    "    r\"\"\"The Variational Graph Auto-Encoder model from the\n",
    "    `\"Variational Graph Auto-Encoders\" <https://arxiv.org/abs/1611.07308>`_\n",
    "    paper.\n",
    "\n",
    "    Args:\n",
    "        encoder (Module): The encoder module to compute :math:`\\mu` and\n",
    "            :math:`\\log\\sigma^2`.\n",
    "        decoder (Module, optional): The decoder module. If set to :obj:`None`,\n",
    "            will default to the\n",
    "            :class:`torch_geometric.nn.models.InnerProductDecoder`.\n",
    "            (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder=None):\n",
    "        super(VGAE, self).__init__(encoder, decoder)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            return mu + torch.randn_like(logvar) * torch.exp(logvar)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def encode(self, *args, **kwargs):\n",
    "        \"\"\"\"\"\"\n",
    "        self.__mu__, self.__logvar__ = self.encoder(*args, **kwargs)\n",
    "        self.__logvar__ = self.__logvar__.clamp(max=MAX_LOGVAR)\n",
    "        z = self.reparametrize(self.__mu__, self.__logvar__)\n",
    "        return z\n",
    "\n",
    "    def kl_loss(self, mu=None, logvar=None):\n",
    "        r\"\"\"Computes the KL loss, either for the passed arguments :obj:`mu`\n",
    "        and :obj:`logvar`, or based on latent variables from last encoding.\n",
    "\n",
    "        Args:\n",
    "            mu (Tensor, optional): The latent space for :math:`\\mu`. If set to\n",
    "                :obj:`None`, uses the last computation of :math:`mu`.\n",
    "                (default: :obj:`None`)\n",
    "            logvar (Tensor, optional): The latent space for\n",
    "                :math:`\\log\\sigma^2`.  If set to :obj:`None`, uses the last\n",
    "                computation of :math:`\\log\\sigma^2`.(default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        mu = self.__mu__ if mu is None else mu\n",
    "        logvar = self.__logvar__ if logvar is None else logvar.clamp(\n",
    "            max=MAX_LOGVAR)\n",
    "        return -0.5 * torch.mean(\n",
    "            torch.sum(1 + logvar - mu**2 - logvar.exp(), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new encoder which incorporates new GCN layers for embedding the probability distribution\n",
    "class VGAEEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(VGAEEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True)\n",
    "        self.conv_mu = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "        self.conv_logvar = GCNConv(2 * out_channels, out_channels, cached=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        return self.conv_mu(x, edge_index), self.conv_logvar(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring the encoder and decoder together to compile the VGAE\n",
    "# in_channels (the dimensionality of the input data) is simply the number of features\n",
    "# we define out_channels (the dimensionality of the latent vector or embedding) as 16\n",
    "vgae = VGAE(VGAEEncoder(in_channels=dataset.num_features, out_channels=16)).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new optimizer with vgae params\n",
    "vgae_optimizer = torch.optim.Adam(vgae.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vgae():\n",
    "    print('training vgae')\n",
    "    auc, ap = None, None\n",
    "    for epoch in range(200):\n",
    "        vgae_optimizer.zero_grad()\n",
    "        # get the embedding (z) of each node, shape(num_nodes, channels)\n",
    "        z = vgae.encode(x, train_pos_edge_index)\n",
    "        # calculate reconstruction loss\n",
    "        recon_loss = vgae.recon_loss(z, train_pos_edge_index)\n",
    "        # calculate kl loss\n",
    "        kl_loss = vgae.kl_loss()\n",
    "        # multiply the two losses, ensuring output is nonzero\n",
    "        loss = recon_loss + (1 / data.num_nodes) * kl_loss\n",
    "        loss.backward()\n",
    "        vgae_optimizer.step()\n",
    "\n",
    "        writer.add_scalar(\"loss\", loss.item(), epoch)\n",
    "\n",
    "        # calculate performance on edge prediction task\n",
    "        auc, ap = test(vgae, data.test_pos_edge_index, data.test_neg_edge_index)\n",
    "        writer.add_scalar(\"AUC\", auc, epoch)\n",
    "        writer.add_scalar(\"AP\", ap, epoch)\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
    "    return auc, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_vgae()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "We will now run each model 10 times over the dataset in order to robustly compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running experiments on PubMed\n",
      "1\n",
      "training gae\n",
      "Epoch: 000, AUC: 0.8927, AP: 0.8780\n",
      "Epoch: 100, AUC: 0.9555, AP: 0.9566\n",
      "training vgae\n",
      "Epoch: 000, AUC: 0.8872, AP: 0.8725\n",
      "Epoch: 100, AUC: 0.9452, AP: 0.9455\n",
      "2\n",
      "training gae\n",
      "Epoch: 000, AUC: 0.9693, AP: 0.9700\n",
      "Epoch: 100, AUC: 0.9724, AP: 0.9731\n",
      "training vgae\n",
      "Epoch: 000, AUC: 0.9600, AP: 0.9605\n",
      "Epoch: 100, AUC: 0.9656, AP: 0.9657\n",
      "3\n",
      "training gae\n",
      "Epoch: 000, AUC: 0.9727, AP: 0.9736\n",
      "Epoch: 100, AUC: 0.9735, AP: 0.9747\n",
      "training vgae\n",
      "Epoch: 000, AUC: 0.9694, AP: 0.9691\n",
      "Epoch: 100, AUC: 0.9720, AP: 0.9721\n",
      "4\n",
      "training gae\n",
      "Epoch: 000, AUC: 0.9739, AP: 0.9757\n",
      "Epoch: 100, AUC: 0.9740, AP: 0.9759\n",
      "training vgae\n",
      "Epoch: 000, AUC: 0.9724, AP: 0.9736\n",
      "Epoch: 100, AUC: 0.9721, AP: 0.9738\n",
      "5\n",
      "training gae\n",
      "Epoch: 000, AUC: 0.9729, AP: 0.9756\n",
      "Epoch: 100, AUC: 0.9717, AP: 0.9750\n",
      "training vgae\n",
      "Epoch: 000, AUC: 0.9717, AP: 0.9738\n",
      "Epoch: 100, AUC: 0.9707, AP: 0.9734\n",
      "6\n",
      "training gae\n",
      "Epoch: 000, AUC: 0.9709, AP: 0.9746\n",
      "Epoch: 100, AUC: 0.9707, AP: 0.9745\n",
      "training vgae\n",
      "Epoch: 000, AUC: 0.9690, AP: 0.9725\n",
      "Epoch: 100, AUC: 0.9691, AP: 0.9726\n",
      "7\n",
      "training gae\n",
      "Epoch: 000, AUC: 0.9699, AP: 0.9742\n",
      "Epoch: 100, AUC: 0.9700, AP: 0.9741\n",
      "training vgae\n",
      "Epoch: 000, AUC: 0.9687, AP: 0.9725\n",
      "Epoch: 100, AUC: 0.9683, AP: 0.9724\n",
      "8\n",
      "training gae\n",
      "Epoch: 000, AUC: 0.9686, AP: 0.9732\n",
      "Epoch: 100, AUC: 0.9691, AP: 0.9734\n",
      "training vgae\n",
      "Epoch: 000, AUC: 0.9682, AP: 0.9724\n",
      "Epoch: 100, AUC: 0.9685, AP: 0.9727\n",
      "9\n",
      "training gae\n",
      "Epoch: 000, AUC: 0.9688, AP: 0.9733\n",
      "Epoch: 100, AUC: 0.9683, AP: 0.9730\n",
      "training vgae\n",
      "Epoch: 000, AUC: 0.9684, AP: 0.9728\n",
      "Epoch: 100, AUC: 0.9682, AP: 0.9728\n",
      "10\n",
      "training gae\n",
      "Epoch: 000, AUC: 0.9681, AP: 0.9728\n",
      "Epoch: 100, AUC: 0.9676, AP: 0.9724\n",
      "training vgae\n",
      "Epoch: 000, AUC: 0.9686, AP: 0.9731\n",
      "Epoch: 100, AUC: 0.9684, AP: 0.9731\n"
     ]
    }
   ],
   "source": [
    "print('running experiments on', dataset_name)\n",
    "gae_results = []\n",
    "vgae_results = []\n",
    "n_experiments = 10\n",
    "for i in range(n_experiments):\n",
    "    print(i + 1)\n",
    "    gae_results.append(train_gae())\n",
    "    vgae_results.append(train_vgae())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAE results\n",
      "AUC: mean = 0.970, std = 0.002\n",
      "AP : mean = 0.973, std = 0.002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('GAE results')\n",
    "gae_aucs, gae_aps = list(zip(*gae_results))\n",
    "print('AUC: mean = %0.3f, std = %0.3f' % (np.mean(gae_aucs), np.std(gae_aucs)))\n",
    "print('AP : mean = %0.3f, std = %0.3f' % (np.mean(gae_aps), np.std(gae_aps)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGAE results\n",
      "AUC: mean = 0.969, std = 0.003\n",
      "AP : mean = 0.971, std = 0.004\n"
     ]
    }
   ],
   "source": [
    "print('VGAE results')\n",
    "vgae_aucs, vgae_aps = list(zip(*vgae_results))\n",
    "print('AUC: mean = %0.3f, std = %0.3f' % (np.mean(vgae_aucs), np.std(vgae_aucs)))\n",
    "print('AP : mean = %0.3f, std = %0.3f' % (np.mean(vgae_aps), np.std(vgae_aps)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
